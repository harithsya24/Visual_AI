{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import fiftyone.utils.huggingface as fouh\n",
        "\n",
        "dataset = fouh.load_from_hub(\"Voxel51/VisDrone2019-DET\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duYV5c4xTT5P",
        "outputId": "79a4eb55-f415-4458-d01b-632c932a9c2d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading config file fiftyone.yml from Voxel51/VisDrone2019-DET\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "INFO:fiftyone.utils.huggingface:Downloading config file fiftyone.yml from Voxel51/VisDrone2019-DET\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.huggingface:Loading dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Importing samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.data.importers:Importing samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |███████████████| 8629/8629 [2.0s elapsed, 0s remaining, 4.8K samples/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |███████████████| 8629/8629 [2.0s elapsed, 0s remaining, 4.8K samples/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Migrating dataset 'Voxel51/VisDrone2019-DET' to v1.0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.migrations.runner:Migrating dataset 'Voxel51/VisDrone2019-DET' to v1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fiftyone as fo\n",
        "import fiftyone.utils.huggingface as fouh"
      ],
      "metadata": {
        "id": "AYSLa1EJKjFA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fiftyone as fo\n",
        "\n",
        "datasets = fo.list_datasets()\n",
        "print(\"Existing Datasets:\", datasets)\n"
      ],
      "metadata": {
        "id": "ks1H72HEJmzt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ce86581-f9e2-4324-a2f5-8ffaca82b987"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Existing Datasets: ['Voxel51/VisDrone2019-DET']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "E2Ig5o-AK1jP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2c687d7-9872-4a15-9f1b-c1079b1a7e3c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fiftyone as fo\n",
        "dataset = fo.load_dataset(\"Voxel51/VisDrone2019-DET\")  #\"Voxel51/VisDrone2019-DET\""
      ],
      "metadata": {
        "id": "fRWsrBgaNagg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Dataset name: {dataset.name}\")\n",
        "print(f\"Number of samples: {len(dataset)}\")\n",
        "\n",
        "for sample in dataset.take(5):\n",
        "    print(sample.id)\n"
      ],
      "metadata": {
        "id": "UQ2zOMiQOAr_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81e4d563-3c64-42e3-df5f-5f55f5a73d70"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset name: Voxel51/VisDrone2019-DET\n",
            "Number of samples: 8629\n",
            "66182184534f2808b7fb7050\n",
            "66182184534f2808b7fb6ea4\n",
            "66182184534f2808b7fb6a85\n",
            "66182184534f2808b7fb794e\n",
            "66182184534f2808b7fb7193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_sample = dataset.first()\n",
        "field_names = first_sample.to_dict().keys()\n",
        "\n",
        "print(\"Field names in the dataset:\", list(field_names))\n"
      ],
      "metadata": {
        "id": "_XWj0DsMONxk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ebdb419-6246-4b84-a48f-823a7862a7ea"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Field names in the dataset: ['filepath', 'tags', 'metadata', 'created_at', 'last_modified_at', 'ground_truth']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sample in dataset:\n",
        "    image_path = sample.filepath\n",
        "    print(f\"Image Path: {image_path}\")\n",
        "\n",
        "    if sample.ground_truth is not None:\n",
        "        detections = sample.ground_truth.detections\n",
        "        for detection in detections:\n",
        "            print(f\"Label: {detection.label}, Bounding Box: {detection.bounding_box}\")\n",
        "    else:\n",
        "        print(\"No ground truth annotations available.\")\n"
      ],
      "metadata": {
        "id": "0kGYdP8oOF0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(f\"Error loading image at {image_path}. Skipping...\")\n",
        "        return None\n",
        "\n",
        "    img = cv2.resize(img, (300, 300))\n",
        "    img = img / 255.0\n",
        "    return img\n",
        "\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "for sample in dataset:\n",
        "    image_path = sample.filepath\n",
        "    img = preprocess_image(image_path)\n",
        "\n",
        "    if img is not None:\n",
        "        images.append(img)\n",
        "\n",
        "        if sample.ground_truth is not None:\n",
        "            detections = sample.ground_truth.detections\n",
        "            label = [detection.label for detection in detections]\n",
        "            labels.append(label)\n",
        "        else:\n",
        "            labels.append([])"
      ],
      "metadata": {
        "id": "ptt3WOFQPNhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "y_encoded = mlb.fit_transform(labels)\n",
        "\n",
        "print(f\"Encoded labels shape: {y_encoded.shape}\")\n"
      ],
      "metadata": {
        "id": "mllDDVZ4U_a7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
        "\n",
        "print(f\"Training data shape: {X_train.shape}, Training labels shape: {y_train.shape}\")\n",
        "print(f\"Testing data shape: {X_test.shape}, Testing labels shape: {y_test.shape}\")\n"
      ],
      "metadata": {
        "id": "bat9gS1EUd9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential([\n",
        "    layers.Input(shape=(300, 300, 3)),\n",
        "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(y_encoded.shape[1], activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "cpNShQCQUZwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "id": "hegg9LoiUgvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_acc}\")"
      ],
      "metadata": {
        "id": "72QyUE-oU1QV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(X_test)\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"Predicted label: {predictions[i]}\")\n"
      ],
      "metadata": {
        "id": "UiUCPzM1U4Mq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}